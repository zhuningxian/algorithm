{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text 之spacy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpFLTbzuT6exfxHvKQqSwF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhuningxian/algorithm/blob/main/text_%E4%B9%8Bspacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "文本分析是一种从文本中提取有用信息的技术，涉及多种技术流派，本书使用自然语言处理（NLP）、计算语言学（CL）和数值工具来实现文本信息的提取。其中，数值工具指的是机器学习算法或信息检索算法。\n",
        "自然语言处理（NLP）指使用计算机处理自然语言。\n",
        "\n",
        "计算语言学（CL），顾名思义，是从计算的角度研究语言学的学科。即使用计算机和算法来执行语言学任务，例如文本的词性标注任务（如名词或动词）是通过算法，而不是人工来完成。\n",
        "机器学习（ML）是一门使用统计算法来指导机器执行特定任务的学科。机器学习过程发生在数据上，常见的场景是基于先前观察到的数据来预测一个新的值。\n"
      ],
      "metadata": {
        "id": "rPAUNBLRIHQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy是一个工业级的自然语言处理库。\n",
        "有很多自然语言处理和机器学习用到的开源库是由高校学者发布和维护的，主要用途是进行科学研究。虽然这些库确实能够满足基本的科研工作，但在诞生之初，它们的设计目标并不是提供工业级的算法实现。\n",
        "NLTK（NatureLanguage ToolKit）就是这样的一个例子，它专注于如何帮助科研工作者和学生们快速上手。spaCy则代表另一种开源库的定位：满足工业级的生产开发。换句话说，它能够运行于现实世界的数据之上，既支持大数据又可扩展。"
      ],
      "metadata": {
        "id": "K9O1ZL8bHseQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFiO-XnD_Lse",
        "outputId": "911bcbee-a0d9-40ca-db06-fe40eda1c51d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('this', 'PRON')\n",
            "('is', 'AUX')\n",
            "('a', 'DET')\n",
            "('sentence', 'NOUN')\n",
            "Microsoft 0 9 ORG\n",
            "Europe 31 37 LOC\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "#spacy download en\n",
        "#nlp=spacy.load('en')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(u'this is a sentence')\n",
        "for token in doc:\n",
        "  print((token.text,token.pos_))\n",
        "\n",
        "#命名实体识别的结果存储于Doc对象的ents属性\n",
        "doc = nlp(u'Microsoft has offices all over Europe.')\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "#停用词是执行文本挖掘或NLP算法之前，需要从文本中预先过滤掉的词。\n",
        "my_stop_words = [u'say', u'be', u'said', u'says', u'saying','field']\n",
        "for stopword in my_stop_words:\n",
        "  lexeme = nlp.vocab[stopword]\n",
        "  lexeme.is_stop = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "停用词"
      ],
      "metadata": {
        "id": "G6pRG7WzDjiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "doc = nlp(u'the horse galloped down the field and past the river.')\n",
        "sentence = []\n",
        "for w in doc:\n",
        "  # if it's not a stop word or punctuation mark, add it to our article!\n",
        "  if w.text != 'n' and not w.is_stop and not w.is_punct and not w.like_num:\n",
        "    # we add the lematized version of the word\n",
        "    sentence.append(w.lemma_)\n",
        "print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLGKEShiCW77",
        "outputId": "1bcd882f-1198-4086-c3c0-9b5461e5c147"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['horse', 'gallop', 'past', 'river']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim：文本向量化、向量变换和n-grams的工具"
      ],
      "metadata": {
        "id": "e9AvDkeJDbs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "documents = [u\"Football club Arsenal defeat local rivals this weekend.\",\n",
        "u\"Weekend football frenzy takes over London.\", u\"Bank open for takeover bids after losing millions.\", \n",
        "u\"London football clubs bid to move to Wembley stadium.\", u\"Arsenal bid 50 million pounds for striker Kane.\",\n",
        "u\"Financial troubles result in loss of millions for bank.\",\n",
        "u\"Western bank files for bankruptcy after financial losses.\", \n",
        "u\"London football club is taken over by oil millionaire from Russia.\",\n",
        "u\"Banking on finances not working for Russia.\"]\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "texts = []\n",
        "for document in documents:\n",
        "  text = []\n",
        "  doc = nlp(document)\n",
        "  for w in doc:\n",
        "    if not w.is_stop and not w.is_punct and not w.like_num:\n",
        "      text.append(w.lemma_)\n",
        "      texts.append(text)\n",
        "print(texts)\n",
        "\n",
        "#Gensim支持Pythondictionary类，可以很方便地完成这一操作\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "print('Pythondictionary类')\n",
        "print(dictionary.token2id)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxL7wqyPC6PX",
        "outputId": "2f1269c5-0b50-4693-a905-b33f77be608b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['football', 'club', 'Arsenal', 'defeat', 'local', 'rival', 'weekend'], ['football', 'club', 'Arsenal', 'defeat', 'local', 'rival', 'weekend'], ['football', 'club', 'Arsenal', 'defeat', 'local', 'rival', 'weekend'], ['football', 'club', 'Arsenal', 'defeat', 'local', 'rival', 'weekend'], ['football', 'club', 'Arsenal', 'defeat', 'local', 'rival', 'weekend'], ['football', 'club', 'Arsenal', 'defeat', 'local', 'rival', 'weekend'], ['football', 'club', 'Arsenal', 'defeat', 'local', 'rival', 'weekend'], ['Weekend', 'football', 'frenzy', 'take', 'London'], ['Weekend', 'football', 'frenzy', 'take', 'London'], ['Weekend', 'football', 'frenzy', 'take', 'London'], ['Weekend', 'football', 'frenzy', 'take', 'London'], ['Weekend', 'football', 'frenzy', 'take', 'London'], ['Bank', 'open', 'takeover', 'bid', 'lose', 'million'], ['Bank', 'open', 'takeover', 'bid', 'lose', 'million'], ['Bank', 'open', 'takeover', 'bid', 'lose', 'million'], ['Bank', 'open', 'takeover', 'bid', 'lose', 'million'], ['Bank', 'open', 'takeover', 'bid', 'lose', 'million'], ['Bank', 'open', 'takeover', 'bid', 'lose', 'million'], ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'], ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'], ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'], ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'], ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'], ['London', 'football', 'club', 'bid', 'Wembley', 'stadium'], ['Arsenal', 'bid', 'pound', 'striker', 'Kane'], ['Arsenal', 'bid', 'pound', 'striker', 'Kane'], ['Arsenal', 'bid', 'pound', 'striker', 'Kane'], ['Arsenal', 'bid', 'pound', 'striker', 'Kane'], ['Arsenal', 'bid', 'pound', 'striker', 'Kane'], ['financial', 'trouble', 'result', 'loss', 'million', 'bank'], ['financial', 'trouble', 'result', 'loss', 'million', 'bank'], ['financial', 'trouble', 'result', 'loss', 'million', 'bank'], ['financial', 'trouble', 'result', 'loss', 'million', 'bank'], ['financial', 'trouble', 'result', 'loss', 'million', 'bank'], ['financial', 'trouble', 'result', 'loss', 'million', 'bank'], ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'], ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'], ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'], ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'], ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'], ['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss'], ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'], ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'], ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'], ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'], ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'], ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'], ['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia'], ['banking', 'finance', 'work', 'Russia'], ['banking', 'finance', 'work', 'Russia'], ['banking', 'finance', 'work', 'Russia'], ['banking', 'finance', 'work', 'Russia']]\n",
            "Pythondictionary类\n",
            "{'Arsenal': 0, 'club': 1, 'defeat': 2, 'football': 3, 'local': 4, 'rival': 5, 'weekend': 6, 'London': 7, 'Weekend': 8, 'frenzy': 9, 'take': 10, 'Bank': 11, 'bid': 12, 'lose': 13, 'million': 14, 'open': 15, 'takeover': 16, 'Wembley': 17, 'stadium': 18, 'Kane': 19, 'pound': 20, 'striker': 21, 'bank': 22, 'financial': 23, 'loss': 24, 'result': 25, 'trouble': 26, 'bankruptcy': 27, 'file': 28, 'western': 29, 'Russia': 30, 'millionaire': 31, 'oil': 32, 'banking': 33, 'finance': 34, 'work': 35}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#doc2bow函数的用法，正如字面描述的那样，它的功能是将文档转换为词袋\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKzk6X3uEdWC",
        "outputId": "6f10ebc3-c09b-46f7-abd3-19ea05ddff1d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(3, 1), (7, 1), (8, 1), (9, 1), (10, 1)], [(3, 1), (7, 1), (8, 1), (9, 1), (10, 1)], [(3, 1), (7, 1), (8, 1), (9, 1), (10, 1)], [(3, 1), (7, 1), (8, 1), (9, 1), (10, 1)], [(3, 1), (7, 1), (8, 1), (9, 1), (10, 1)], [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)], [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)], [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)], [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)], [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)], [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)], [(1, 1), (3, 1), (7, 1), (12, 1), (17, 1), (18, 1)], [(1, 1), (3, 1), (7, 1), (12, 1), (17, 1), (18, 1)], [(1, 1), (3, 1), (7, 1), (12, 1), (17, 1), (18, 1)], [(1, 1), (3, 1), (7, 1), (12, 1), (17, 1), (18, 1)], [(1, 1), (3, 1), (7, 1), (12, 1), (17, 1), (18, 1)], [(1, 1), (3, 1), (7, 1), (12, 1), (17, 1), (18, 1)], [(0, 1), (12, 1), (19, 1), (20, 1), (21, 1)], [(0, 1), (12, 1), (19, 1), (20, 1), (21, 1)], [(0, 1), (12, 1), (19, 1), (20, 1), (21, 1)], [(0, 1), (12, 1), (19, 1), (20, 1), (21, 1)], [(0, 1), (12, 1), (19, 1), (20, 1), (21, 1)], [(14, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)], [(14, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)], [(14, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)], [(14, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)], [(14, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)], [(14, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)], [(22, 1), (23, 1), (24, 1), (27, 1), (28, 1), (29, 1)], [(22, 1), (23, 1), (24, 1), (27, 1), (28, 1), (29, 1)], [(22, 1), (23, 1), (24, 1), (27, 1), (28, 1), (29, 1)], [(22, 1), (23, 1), (24, 1), (27, 1), (28, 1), (29, 1)], [(22, 1), (23, 1), (24, 1), (27, 1), (28, 1), (29, 1)], [(22, 1), (23, 1), (24, 1), (27, 1), (28, 1), (29, 1)], [(1, 1), (3, 1), (7, 1), (10, 1), (30, 1), (31, 1), (32, 1)], [(1, 1), (3, 1), (7, 1), (10, 1), (30, 1), (31, 1), (32, 1)], [(1, 1), (3, 1), (7, 1), (10, 1), (30, 1), (31, 1), (32, 1)], [(1, 1), (3, 1), (7, 1), (10, 1), (30, 1), (31, 1), (32, 1)], [(1, 1), (3, 1), (7, 1), (10, 1), (30, 1), (31, 1), (32, 1)], [(1, 1), (3, 1), (7, 1), (10, 1), (30, 1), (31, 1), (32, 1)], [(1, 1), (3, 1), (7, 1), (10, 1), (30, 1), (31, 1), (32, 1)], [(30, 1), (33, 1), (34, 1), (35, 1)], [(30, 1), (33, 1), (34, 1), (35, 1)], [(30, 1), (33, 1), (34, 1), (35, 1)], [(30, 1), (33, 1), (34, 1), (35, 1)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import models\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "for document in tfidf[corpus]:\n",
        "  print(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsA8rr0hFY9K",
        "outputId": "67b1c46f-6c82-484c-ecd6-d2e22f97740f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0.3304962557174551), (1, 0.2153617756772611), (2, 0.45198014174070267), (3, 0.16506767225019192), (4, 0.45198014174070267), (5, 0.45198014174070267), (6, 0.45198014174070267)]\n",
            "[(0, 0.3304962557174551), (1, 0.2153617756772611), (2, 0.45198014174070267), (3, 0.16506767225019192), (4, 0.45198014174070267), (5, 0.45198014174070267), (6, 0.45198014174070267)]\n",
            "[(0, 0.3304962557174551), (1, 0.2153617756772611), (2, 0.45198014174070267), (3, 0.16506767225019192), (4, 0.45198014174070267), (5, 0.45198014174070267), (6, 0.45198014174070267)]\n",
            "[(0, 0.3304962557174551), (1, 0.2153617756772611), (2, 0.45198014174070267), (3, 0.16506767225019192), (4, 0.45198014174070267), (5, 0.45198014174070267), (6, 0.45198014174070267)]\n",
            "[(0, 0.3304962557174551), (1, 0.2153617756772611), (2, 0.45198014174070267), (3, 0.16506767225019192), (4, 0.45198014174070267), (5, 0.45198014174070267), (6, 0.45198014174070267)]\n",
            "[(0, 0.3304962557174551), (1, 0.2153617756772611), (2, 0.45198014174070267), (3, 0.16506767225019192), (4, 0.45198014174070267), (5, 0.45198014174070267), (6, 0.45198014174070267)]\n",
            "[(0, 0.3304962557174551), (1, 0.2153617756772611), (2, 0.45198014174070267), (3, 0.16506767225019192), (4, 0.45198014174070267), (5, 0.45198014174070267), (6, 0.45198014174070267)]\n",
            "[(3, 0.19049830700281703), (7, 0.27594643920373124), (8, 0.6091338045126214), (9, 0.6091338045126214), (10, 0.38141312788078097)]\n",
            "[(3, 0.19049830700281703), (7, 0.27594643920373124), (8, 0.6091338045126214), (9, 0.6091338045126214), (10, 0.38141312788078097)]\n",
            "[(3, 0.19049830700281703), (7, 0.27594643920373124), (8, 0.6091338045126214), (9, 0.6091338045126214), (10, 0.38141312788078097)]\n",
            "[(3, 0.19049830700281703), (7, 0.27594643920373124), (8, 0.6091338045126214), (9, 0.6091338045126214), (10, 0.38141312788078097)]\n",
            "[(3, 0.19049830700281703), (7, 0.27594643920373124), (8, 0.6091338045126214), (9, 0.6091338045126214), (10, 0.38141312788078097)]\n",
            "[(11, 0.4598434935647969), (12, 0.23807489843335386), (13, 0.4598434935647969), (14, 0.3122437964803119), (15, 0.4598434935647969), (16, 0.4598434935647969)]\n",
            "[(11, 0.4598434935647969), (12, 0.23807489843335386), (13, 0.4598434935647969), (14, 0.3122437964803119), (15, 0.4598434935647969), (16, 0.4598434935647969)]\n",
            "[(11, 0.4598434935647969), (12, 0.23807489843335386), (13, 0.4598434935647969), (14, 0.3122437964803119), (15, 0.4598434935647969), (16, 0.4598434935647969)]\n",
            "[(11, 0.4598434935647969), (12, 0.23807489843335386), (13, 0.4598434935647969), (14, 0.3122437964803119), (15, 0.4598434935647969), (16, 0.4598434935647969)]\n",
            "[(11, 0.4598434935647969), (12, 0.23807489843335386), (13, 0.4598434935647969), (14, 0.3122437964803119), (15, 0.4598434935647969), (16, 0.4598434935647969)]\n",
            "[(11, 0.4598434935647969), (12, 0.23807489843335386), (13, 0.4598434935647969), (14, 0.3122437964803119), (15, 0.4598434935647969), (16, 0.4598434935647969)]\n",
            "[(1, 0.2634798541229414), (3, 0.20194858659630824), (7, 0.29253274871704527), (12, 0.3082940361603536), (17, 0.5954722970210771), (18, 0.5954722970210771)]\n",
            "[(1, 0.2634798541229414), (3, 0.20194858659630824), (7, 0.29253274871704527), (12, 0.3082940361603536), (17, 0.5954722970210771), (18, 0.5954722970210771)]\n",
            "[(1, 0.2634798541229414), (3, 0.20194858659630824), (7, 0.29253274871704527), (12, 0.3082940361603536), (17, 0.5954722970210771), (18, 0.5954722970210771)]\n",
            "[(1, 0.2634798541229414), (3, 0.20194858659630824), (7, 0.29253274871704527), (12, 0.3082940361603536), (17, 0.5954722970210771), (18, 0.5954722970210771)]\n",
            "[(1, 0.2634798541229414), (3, 0.20194858659630824), (7, 0.29253274871704527), (12, 0.3082940361603536), (17, 0.5954722970210771), (18, 0.5954722970210771)]\n",
            "[(1, 0.2634798541229414), (3, 0.20194858659630824), (7, 0.29253274871704527), (12, 0.3082940361603536), (17, 0.5954722970210771), (18, 0.5954722970210771)]\n",
            "[(0, 0.32910039462188206), (12, 0.2509274608724581), (19, 0.525588032474048), (20, 0.525588032474048), (21, 0.525588032474048)]\n",
            "[(0, 0.32910039462188206), (12, 0.2509274608724581), (19, 0.525588032474048), (20, 0.525588032474048), (21, 0.525588032474048)]\n",
            "[(0, 0.32910039462188206), (12, 0.2509274608724581), (19, 0.525588032474048), (20, 0.525588032474048), (21, 0.525588032474048)]\n",
            "[(0, 0.32910039462188206), (12, 0.2509274608724581), (19, 0.525588032474048), (20, 0.525588032474048), (21, 0.525588032474048)]\n",
            "[(0, 0.32910039462188206), (12, 0.2509274608724581), (19, 0.525588032474048), (20, 0.525588032474048), (21, 0.525588032474048)]\n",
            "[(14, 0.34631882836207556), (22, 0.34631882836207556), (23, 0.34631882836207556), (24, 0.34631882836207556), (25, 0.5100260172224927), (26, 0.5100260172224927)]\n",
            "[(14, 0.34631882836207556), (22, 0.34631882836207556), (23, 0.34631882836207556), (24, 0.34631882836207556), (25, 0.5100260172224927), (26, 0.5100260172224927)]\n",
            "[(14, 0.34631882836207556), (22, 0.34631882836207556), (23, 0.34631882836207556), (24, 0.34631882836207556), (25, 0.5100260172224927), (26, 0.5100260172224927)]\n",
            "[(14, 0.34631882836207556), (22, 0.34631882836207556), (23, 0.34631882836207556), (24, 0.34631882836207556), (25, 0.5100260172224927), (26, 0.5100260172224927)]\n",
            "[(14, 0.34631882836207556), (22, 0.34631882836207556), (23, 0.34631882836207556), (24, 0.34631882836207556), (25, 0.5100260172224927), (26, 0.5100260172224927)]\n",
            "[(14, 0.34631882836207556), (22, 0.34631882836207556), (23, 0.34631882836207556), (24, 0.34631882836207556), (25, 0.5100260172224927), (26, 0.5100260172224927)]\n",
            "[(22, 0.32433030192837264), (23, 0.32433030192837264), (24, 0.32433030192837264), (27, 0.4776433696644223), (28, 0.4776433696644223), (29, 0.4776433696644223)]\n",
            "[(22, 0.32433030192837264), (23, 0.32433030192837264), (24, 0.32433030192837264), (27, 0.4776433696644223), (28, 0.4776433696644223), (29, 0.4776433696644223)]\n",
            "[(22, 0.32433030192837264), (23, 0.32433030192837264), (24, 0.32433030192837264), (27, 0.4776433696644223), (28, 0.4776433696644223), (29, 0.4776433696644223)]\n",
            "[(22, 0.32433030192837264), (23, 0.32433030192837264), (24, 0.32433030192837264), (27, 0.4776433696644223), (28, 0.4776433696644223), (29, 0.4776433696644223)]\n",
            "[(22, 0.32433030192837264), (23, 0.32433030192837264), (24, 0.32433030192837264), (27, 0.4776433696644223), (28, 0.4776433696644223), (29, 0.4776433696644223)]\n",
            "[(22, 0.32433030192837264), (23, 0.32433030192837264), (24, 0.32433030192837264), (27, 0.4776433696644223), (28, 0.4776433696644223), (29, 0.4776433696644223)]\n",
            "[(1, 0.24524026488825146), (3, 0.18796854520641074), (7, 0.27228195120520204), (10, 0.37634807310553636), (30, 0.39868029450620784), (31, 0.5146861802013957), (32, 0.5146861802013957)]\n",
            "[(1, 0.24524026488825146), (3, 0.18796854520641074), (7, 0.27228195120520204), (10, 0.37634807310553636), (30, 0.39868029450620784), (31, 0.5146861802013957), (32, 0.5146861802013957)]\n",
            "[(1, 0.24524026488825146), (3, 0.18796854520641074), (7, 0.27228195120520204), (10, 0.37634807310553636), (30, 0.39868029450620784), (31, 0.5146861802013957), (32, 0.5146861802013957)]\n",
            "[(1, 0.24524026488825146), (3, 0.18796854520641074), (7, 0.27228195120520204), (10, 0.37634807310553636), (30, 0.39868029450620784), (31, 0.5146861802013957), (32, 0.5146861802013957)]\n",
            "[(1, 0.24524026488825146), (3, 0.18796854520641074), (7, 0.27228195120520204), (10, 0.37634807310553636), (30, 0.39868029450620784), (31, 0.5146861802013957), (32, 0.5146861802013957)]\n",
            "[(1, 0.24524026488825146), (3, 0.18796854520641074), (7, 0.27228195120520204), (10, 0.37634807310553636), (30, 0.39868029450620784), (31, 0.5146861802013957), (32, 0.5146861802013957)]\n",
            "[(1, 0.24524026488825146), (3, 0.18796854520641074), (7, 0.27228195120520204), (10, 0.37634807310553636), (30, 0.39868029450620784), (31, 0.5146861802013957), (32, 0.5146861802013957)]\n",
            "[(30, 0.330053323265056), (33, 0.5449968818876223), (34, 0.5449968818876223), (35, 0.5449968818876223)]\n",
            "[(30, 0.330053323265056), (33, 0.5449968818876223), (34, 0.5449968818876223), (35, 0.5449968818876223)]\n",
            "[(30, 0.330053323265056), (33, 0.5449968818876223), (34, 0.5449968818876223), (35, 0.5449968818876223)]\n",
            "[(30, 0.330053323265056), (33, 0.5449968818876223), (34, 0.5449968818876223), (35, 0.5449968818876223)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "n-grams及其预处理技术\n",
        "有些向量表示法会在执行过程中丢失上下文，如词袋模型只保留了每个单词的词频。\n",
        "n-grams，尤其是bi-grams，能够在某种程度上帮助我们解决这个问题"
      ],
      "metadata": {
        "id": "I0cn_wATF5mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "bigram = gensim.models.Phrases(texts)\n",
        "#我们基于语料库得到了一个训练好的bi-grams模型。执行与TF-IDF类似的变换过程，语料库会被重建为\n",
        "texts = [bigram[line] for line in texts]\n",
        "\n",
        "#因为有新的短语被模型创建，并加入到词汇表中，所以必须在创建字典之前执行以下代码\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6dRqWt-GEDB",
        "outputId": "24780ab7-cc29-4d8c-f5f7-c2f750ea78f2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "目前业界流行的预处理技术是：只剔除高频词和低频词，由dictionary模块实现。比如，我们希望删除出现在少于20篇文档或超过50%的文档中的单词，只需要执行如下代码"
      ],
      "metadata": {
        "id": "7ceVrhNAGhdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary.filter_extremes(no_below=20, no_above=0.5)"
      ],
      "metadata": {
        "id": "jkKrvwepGlpD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "词性标注的全称为Part-Of-Speech tagging。顾名思义，词性标注是为输入文本中的单词标注对应词性的过程,\n",
        "spaCy早期版本的词性标注器使用的就是平均感知机器（averaged perceptron）\n",
        "\n",
        "最具代表性的英语语语料库之一布朗语料库（Brown）正是布朗大学建立的"
      ],
      "metadata": {
        "id": "LGcYYYRpG13E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text = nltk.word_tokenize(\"And now for something completely different\")\n",
        "nltk.pos_tag(text)"
      ],
      "metadata": {
        "id": "9KT5QbVyHKRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "文本聚类和文本分类\n",
        "使用流行的Python机器学习库scikit-learn来执行这些任务\n",
        "\n",
        "聚类是将同一组中的数据点分组或聚类的任务，其中同一组中的点比其他组中的点更相似。\n",
        "一个著名的聚类或分类任务的数据集叫作Iris，该数据集包含花的花瓣长度和类别信息。\n",
        "另一个非常流行的数据集叫作MNIST，它包含手写数字，这些数字应该按照它所代表的数字进行分类。\n",
        "\n",
        "文本聚类遵循标准聚类问题所遵循的大多数原则，但是文本分析领域的维数实在太多了。例如，在Iris数据集中，只有4个特征可以用来标识类或集群。而对于文本，在对问题进行建模时，我们必须处理整个词汇表。当然，我们将尽力使用一些技术，如SVD、LDA和LSI来减\n",
        "少维度。\n",
        "\n",
        "选择目前最流行的20个新闻组数据集。由于数据集本身内置于scikit-learn之\n",
        "中，所以加载和使用也很方便"
      ],
      "metadata": {
        "id": "XrjA5rprJHni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "categories = [\n",
        "'alt.atheism',\n",
        "'talk.religion.misc',\n",
        "'comp.graphics',\n",
        "'sci.space',\n",
        "]\n",
        "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
        "shuffle=True, random_state=42)\n",
        "labels = dataset.target\n",
        "true_k = np.unique(labels).shape[0]\n",
        "data = dataset.data\n",
        "\n",
        "#只选取了4个类别。通过选择所有子集来创建数据集，同时也对数据集进行梳理，保证其状态随机\n",
        "\n",
        "#使用的是scikit-learn内置的TfidfVectorizer类来简化工作\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2,\n",
        "stop_words='english',\n",
        "use_idf=True)\n",
        "X = vectorizer.fit_transform(data)"
      ],
      "metadata": {
        "id": "mv6NC5FjJvDD"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X对象是输入向量，包含数据集的TF-IDF表示。在TF-IDF转换时，我们处理的仍是高维数据。为了更好地理解数据的性质，我们将其进行可视化处理。我们可以使用PCA（主成分分析）将数据集中的数据映射到二维空间"
      ],
      "metadata": {
        "id": "--hdPDs_J5FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "\n",
        "news = fetch_20newsgroups(subset='all')\n",
        "print(len(news.data))\n",
        "print(news.data[0])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(news.data,news.target,test_size=0.25,random_state=33)\n",
        "\n",
        "\n",
        " \n",
        "count_filter_vec = CountVectorizer(analyzer='word',stop_words='english') \n",
        "tfidf_filter_vec = TfidfVectorizer(analyzer='word',stop_words='english') \n",
        "X_count_filter_train = count_filter_vec.fit_transform(X_train)\n",
        "X_count_filter_test = count_filter_vec.transform(X_test) \n",
        "X_tfidf_filter_train = tfidf_filter_vec.fit_transform(X_train)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "#类调用\n",
        "transformer = TfidfTransformer()\n",
        "\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',categories=['alt.atheism', 'sci.space'])\n",
        "pipeline = Pipeline([('vect',CountVectorizer()),('tfidf',TfidfTransformer())])\n",
        "X_visualise =pipeline.fit_transform(newsgroups_train.data).todense()\n",
        "pca = PCA(n_components=2).fit(X_visualise)\n",
        "data2D = pca.transform(X_visualise)\n",
        "plt.scatter(data2D[:,0], data2D[:,1], c=newsgroups_train.target)"
      ],
      "metadata": {
        "id": "zI_06Q3LLBt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "对数据集执行SVD操作之后还需要进行归一化处理"
      ],
      "metadata": {
        "id": "E-fK6VkpNqyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import Pipeline as make_pipeline\n",
        "\n",
        "n_components = 5\n",
        "svd = TruncatedSVD(n_components)\n",
        "normalizer = Normalizer(copy=False)\n",
        "lsa = make_pipeline(svd,normalizer)\n",
        "X = lsa.fit_transform(X)"
      ],
      "metadata": {
        "id": "bs9-HDd1Rrtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "用scikit-learn实现K-means非常简单，scikit-learn库提供了两\n",
        "种实现方式，一种是标准K-means，另一种是小批量K-means"
      ],
      "metadata": {
        "id": "7m8oah7iasee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MiniBatchKMeans, KMeans  \n",
        "minibatch = True\n",
        "if minibatch:\n",
        "  km = MiniBatchKMeans(n_clusters=true_k, init='k-means++',n_init=1,init_size=1000, batch_size=1000)\n",
        "else:\n",
        "  km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100,n_init=1)\n",
        "km.fit(X)"
      ],
      "metadata": {
        "id": "IyCHDkXBaudu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过执行fit函数，我们训练出了4个不同的聚类。之前我们可视化了聚类结果，这里只把每个类别的主题词打印出来"
      ],
      "metadata": {
        "id": "KSiwBSz0c5NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import Pipeline as make_pipeline\n",
        "\n",
        "original_space_centroids=svd.inverse_transform(km.cluster_centers_)\n",
        "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names()\n",
        "for i in range(true_k):\n",
        "  print(\"Cluster %d:\" % i)\n",
        "for ind in order_centroids[i, :10]:\n",
        "  print(' %s' % terms[ind])"
      ],
      "metadata": {
        "id": "tN4GvmOec6ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "文本分类\n",
        "聚类是一种无监督的学习算法。\n",
        "分类是一种有监督的学习算法。\n",
        "使用NaiveBayes分类器和支持向量机分类器来辅助完成分类任务。"
      ],
      "metadata": {
        "id": "DyusQqu3g_8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "iris = datasets.load_iris()\n",
        "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
        "svc = svm.SVC()\n",
        "clf = GridSearchCV(svc, parameters)\n",
        "clf.fit(iris.data, iris.target)\n",
        "\n",
        "#gnb = GaussianNB()\n",
        "#gnb.fit(X,labels)\n",
        "\n",
        "#svm = SVC()\n",
        "#svm.fit(X,labels)\n",
        "#svm.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IROWnaoQJKj_",
        "outputId": "5238a5b0-4cc1-4883-bfdc-c10af6daef37"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=SVC(),\n",
              "             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "查询词相似度计算和文本摘要\n",
        "\n",
        "以向量形式表示文本文档，就可以开始计算文档之间的相似性或距离\n",
        "\n",
        "相似性度量；\n",
        "查询词相似度计算；\n",
        "文本摘要。\n",
        "\n",
        "Gensim（以及scikit-learn等绝大多数机器学习算法包）对各类距离的计算都有现成的实现"
      ],
      "metadata": {
        "id": "ZXYdmovkNlbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "texts = [['bank','river','shore','water'],\n",
        "['river','water','flow','fast','tree'],\n",
        "['bank','water','fall','flow'],\n",
        "['bank','bank','water','rain','river'],\n",
        "['river','water','mud','tree'],\n",
        "['money','transaction','bank','finance'],\n",
        "['bank','borrow','money'],\n",
        "['bank','finance'],\n",
        "['finance','money','sell','bank'],\n",
        "['borrow','sell'],\n",
        "['bank','loan','sell']]\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "#dictionary = Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "#为语料库创建TF-IDF和LDA模型，用于距离计算。\n",
        "from gensim.models import ldamodel\n",
        "from gensim.models import TfidfModel\n",
        "tfidf = TfidfModel(corpus)\n",
        "model = ldamodel.LdaModel(corpus, id2word=dictionary,num_topics=2)\n",
        "\n",
        "model.show_topics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYy53H2ZiaSj",
        "outputId": "c5e9b0a0-5624-4414-c4b5-c1c350aa82df"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.162*\"bank\" + 0.117*\"water\" + 0.097*\"river\" + 0.088*\"finance\" + 0.072*\"flow\" + 0.068*\"tree\" + 0.065*\"money\" + 0.046*\"transaction\" + 0.043*\"shore\" + 0.043*\"fall\"'),\n",
              " (1,\n",
              "  '0.196*\"bank\" + 0.110*\"sell\" + 0.096*\"water\" + 0.081*\"river\" + 0.078*\"money\" + 0.075*\"borrow\" + 0.054*\"finance\" + 0.050*\"rain\" + 0.047*\"loan\" + 0.038*\"tree\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "比较下面3篇文档，第一篇描述river bank，第二篇描述financial bank，第三篇同时与上述两个主题相关（可能financialbank刚好位于river bank上？）"
      ],
      "metadata": {
        "id": "ZmETrl3aNoD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_water = ['river', 'water', 'shore']\n",
        "doc_finance = ['finance', 'money', 'sell']\n",
        "doc_bank = ['finance', 'bank', 'tree', 'water']\n",
        "\n",
        "#下面的代码把3篇文档分别转化为词袋、TF-IDF或者LDA表示。\n",
        "bow_water = model.id2word.doc2bow(doc_water)\n",
        "bow_finance = model.id2word.doc2bow(doc_finance)\n",
        "bow_bank = model.id2word.doc2bow(doc_bank)\n",
        "lda_bow_water = model[bow_water]\n",
        "print('lda_bow_water的结果')\n",
        "print(lda_bow_water)\n",
        "\n",
        "lda_bow_finance = model[bow_finance]\n",
        "print('lda_bow_finance的结果')\n",
        "print(lda_bow_finance)\n",
        "lda_bow_bank = model[bow_bank]\n",
        "print('lda_bow_bank的结果')\n",
        "print(lda_bow_bank)\n",
        "tfidf_bow_water = tfidf[bow_water]\n",
        "tfidf_bow_finance = tfidf[bow_finance]\n",
        "tfidf_bow_bank = tfidf[bow_bank]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz6KM-qXjW7Z",
        "outputId": "b47c2ddd-f77e-4096-fdbc-902690f5007f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lda_bow_water的结果\n",
            "[(0, 0.8260013), (1, 0.17399871)]\n",
            "lda_bow_finance的结果\n",
            "[(0, 0.24967475), (1, 0.75032526)]\n",
            "lda_bow_bank的结果\n",
            "[(0, 0.85140514), (1, 0.14859483)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lda_bow_water的值：\n",
        "[(0, 0.8225102558524345), (1, 0.17748974414756546)]\n",
        "结果较为合理，这篇文档包含与river banks有关的单词，属于topic_0主题的概率为82%。而lda_bow_finance的值则相差许多：\n",
        "[(0, 0.14753674420005805), (1, 0.852463255799942)]\n",
        "lda_bow_bank的值：\n",
        "[(0, 0.44153395450870797), (1, 0.558466045491292)]\n",
        "两个主题的分布概率比较接近。"
      ],
      "metadata": {
        "id": "jUxBMeHojjRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "引入Hellinger距离、Kullback-Leibler距离，以及\n",
        "Jaccard距离。前两个度量方法用于计算两种概率分布的相似或不同程\n",
        "度。"
      ],
      "metadata": {
        "id": "YFQFLj46kdAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.matutils import kullback_leibler, jaccard, hellinger\n",
        "print('lda_bow_water, lda_bow_finance的相似度')\n",
        "print(hellinger(lda_bow_water, lda_bow_finance))\n",
        "print('lda_bow_finance, lda_bow_bank的相似度')\n",
        "print(hellinger(lda_bow_finance, lda_bow_bank))\n",
        "print('lda_bow_bank, lda_bow_water的相似度')\n",
        "print(hellinger(lda_bow_bank, lda_bow_water))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik5qjp_Rkgel",
        "outputId": "3bc5759d-6cfb-4c94-e35d-48374f86b8af"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lda_bow_water, lda_bow_finance的相似度\n",
            "0.4295902969615133\n",
            "lda_bow_finance, lda_bow_bank的相似度\n",
            "0.4528071221606102\n",
            "lda_bow_bank, lda_bow_water的相似度\n",
            "0.02443562568335135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这些距离值都很直观，你会发现对于finance和water这两篇文档而言，Hellinger距离给出的是正确结果，因为这两篇文档相关性并不大。因为bank文档同时包含finance和river的内容，所以与另外两篇文档的距离并不是特别大。其中bank与water较bank与finance之间的\n",
        "距离更远（0.287对0.234）。距离值范围从0到1，其中0表示两者之间不存在距离，0.5可以直观地理解为介于两者之间，而1则表示两者完全相同。在本例中，lda_bow_bank与finance的距离比与water的距离更近。"
      ],
      "metadata": {
        "id": "ub1WbVrMlHIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XFabHrHelLdp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}